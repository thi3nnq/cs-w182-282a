{"cells":[{"cell_type":"markdown","metadata":{"id":"e1tA4eBQ-p7H"},"source":["<center><h3>**Welcome to the Knowledge Distillation Notebook.**</h3></center>\n","\n","This notebook is an experimental part of the homework and not worth points. It is not guaranteed to work correctly. \n","\n","A trend in Natural Language Processing is to pretrain large models that can then be fine-tuned for specific problems. However the state-of-the-art models can be quite large: the \"base\" BERT model has 110M parameters and the \"large\" BERT model has 350M parameters! In many applications, such as client-side mobile apps, we do not have the compute to run the BERT model even in an evaluation setting.\n","\n","Here we look at a method for reducing model size, called Knowledge Distillation. Specifically, we will follow the paper __[TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)__.\n","In this assignment you will:\n","- Use an off-the-shelf API to replicate a paper method\n","- Implement loss functions for KD as specified by the paper\n","\n","**Before You Get Started**\n","\n","Read the Paper. Also, the API we will be using is the Transformers API released by HuggingFace. It may be helpful to look at the __[documentation](https://huggingface.co/transformers/)__."]},{"cell_type":"markdown","metadata":{"id":"xQIyP__s-p7g"},"source":["# Library Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":25151,"status":"ok","timestamp":1640767487133,"user":{"displayName":"Thien Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD4m-NEl2Tp-M42VJnqDgmCteI2aWgkIQGvyxJ=s64","userId":"12631676377868622065"},"user_tz":-420},"id":"UO1rSB3s_Ed5"},"outputs":[],"source":["#block the output to keep not pollute the notebook\n","%%capture \n","!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29828,"status":"ok","timestamp":1640767516946,"user":{"displayName":"Thien Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD4m-NEl2Tp-M42VJnqDgmCteI2aWgkIQGvyxJ=s64","userId":"12631676377868622065"},"user_tz":-420},"id":"8frISYlT_Lri","outputId":"50df32e7-fad0-4f7f-923a-3c36fa28776b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","DRIVE=True"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1640767516947,"user":{"displayName":"Thien Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD4m-NEl2Tp-M42VJnqDgmCteI2aWgkIQGvyxJ=s64","userId":"12631676377868622065"},"user_tz":-420},"id":"YgPvRlHl-p7j"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7526,"status":"ok","timestamp":1640767524468,"user":{"displayName":"Thien Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD4m-NEl2Tp-M42VJnqDgmCteI2aWgkIQGvyxJ=s64","userId":"12631676377868622065"},"user_tz":-420},"id":"2Ih0h2mU-p7m","outputId":"b5ec9c86-5075-465c-831b-f62eb024e68f"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"source":["root_folder = \"\" if not DRIVE else \"/content/drive/My Drive/cs182_hw3_public/\"\n","import os\n","import sys\n","sys.path.append(root_folder)\n","import json\n","from utils import validate_to_array, model_out_to_list\n","import torch as th\n","from torch.nn import functional as F\n","from torch import nn\n","from torch import optim\n","import numpy as np\n","import math\n","device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n","# device = th.device(\"cpu\")\n","print(device)\n","\n","from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig, BertForPreTraining"]},{"cell_type":"markdown","metadata":{"id":"2Sqe9HiM-p7q"},"source":["# BERT Architecture"]},{"cell_type":"markdown","metadata":{"id":"V6PbKRQD-p7s"},"source":["First load the BERT base model and take a look at the architecture. Don't mind the warnings for now. Based on the nn.Module names, what major component from the Transformer architecture in \"Attention is All You Need\" is substantially smaller in the BERT model? What is the purpose of the component?"]},{"cell_type":"markdown","metadata":{"id":"l0dtE6Y0-p7t"},"source":["Answer:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjJWcena-p7v"},"outputs":[],"source":["%%capture\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n","teacher_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRp-Utp7-p7x"},"outputs":[],"source":["print(teacher_model)"]},{"cell_type":"markdown","metadata":{"id":"u3vhv5zl-p7z"},"source":["# KD Losses"]},{"cell_type":"markdown","metadata":{"id":"A5AUU7sv-p70"},"source":["First, we need to access the intermediate layer outputs of the model. Read section 3 of the paper and take a look at the documentation for __[the forward function](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel.forward)__, or look at the docstring below. Fill in the kwargs to retrieve the necessary outputs from the model. Note that the returning the embedding is not an option, you can retrieve the embeddings via a method attribute of BERT, `get_input_embeddings(self)`. Consider what an Embedding is; why wouldn't we need to return an embedding for every sample in a batch?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-nAHNNw-p72"},"outputs":[],"source":["forward_kwargs = dict(\n","    \n","    \n","    \n","    \n","    \n","    return_dict=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rP1TlYYJ-p73"},"outputs":[],"source":["help(teacher_model.forward)"]},{"cell_type":"markdown","metadata":{"id":"FEgc5DzN-p75"},"source":["Implement to EmbeddingLayerLoss, AttentionLayerLoss, HiddenLayerLoss, PredictionLoss, and KnowledgeDistillationLoss as specified in section 3 of the paper. The output of BERT will be a dictionary, look at 'return' in the documentation or the docstring for the relevant keys in the return dictionary. We will add the embedding in under 'embeddings'"]},{"cell_type":"markdown","metadata":{"id":"8SNGTQXNAz6E"},"source":["## (1) Implementing the Attention Layer Loss"]},{"cell_type":"markdown","metadata":{"id":"RKVTOLVwBEv7"},"source":["This part is located in AttentionLayerLoss in kd_loss.py. You must implement the call function of the class. You will need to implement the formula (7) in section 3 of __[TinyBERT](https://arxiv.org/pdf/1909.10351.pdf)__. Note that the actual implemetation compares raw output from attention. The transformers API returns the softmax output."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":881,"status":"ok","timestamp":1640767592487,"user":{"displayName":"Thien Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD4m-NEl2Tp-M42VJnqDgmCteI2aWgkIQGvyxJ=s64","userId":"12631676377868622065"},"user_tz":-420},"id":"lNdxuQ2v6vDy","outputId":"e33e85b5-ad93-410b-a3ee-2370e063b803"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total error on the output: 1.763885498046875 (should be 0.0 or close to 0.0)\n"]}],"source":["from kd_loss import AttentionLayerLoss\n","num_channels = 10\n","batch_size = 2\n","\n","\n","with open(root_folder+\"kd_checks/kd_attention_loss.json\",'r') as f:\n","  io = json.load(f)\n","  teacher_attn = th.tensor(io['teacher_attention'])\n","  student_attn = th.tensor(io['student_attention'])\n","  expected_output = th.tensor(io['expected_output'])\n","\n","attn_loss = AttentionLayerLoss()\n","output = attn_loss(teacher_attn, student_attn)\n","validate_to_array(model_out_to_list,((teacher_attn,student_attn),attn_loss),'kdattnloss', root_folder)\n","print(\"Total error on the output:\",th.sum(th.abs(expected_output-output)).item(), \"(should be 0.0 or close to 0.0)\")"]},{"cell_type":"markdown","metadata":{"id":"yTPdLyBTd1Vq"},"source":["## (2) Implementing the Hidden Layer Loss"]},{"cell_type":"markdown","metadata":{"id":"nDnqWEcEd1WD"},"source":["This part is located in HiddenLayerLoss in kd_loss.py. You must implement the call function of the class. You will need to implement the formula (8) in section 3 of __[TinyBERT](https://arxiv.org/pdf/1909.10351.pdf)__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1itJLead1WG"},"outputs":[],"source":["from kd_loss import HiddenLayerLoss\n","teacher_hidden_dim = 50\n","student_hidden_dim = 10\n","batch_size = 2\n","\n","with open(root_folder+\"kd_checks/kd_hidden_loss.json\",'r') as f:\n","  io = json.load(f)\n","  teacher_hddn = th.tensor(io['teacher_hidden'])\n","  student_hddn = th.tensor(io['student_hidden'])\n","  expected_output = th.tensor(io['expected_output'])\n","\n","hddn_loss = HiddenLayerLoss(teacher_hidden_dim,student_hidden_dim)\n","hddn_loss.load_state_dict(th.load(root_folder+\"kd_checks/kd_hidden_loss\"))\n","output = hddn_loss(teacher_hddn, student_hddn)\n","validate_to_array(model_out_to_list,((teacher_hddn,student_hddn),hddn_loss),'kdhddnloss', root_folder)\n","print(\"Total error on the output:\",th.sum(th.abs(expected_output-output)).item(), \"(should be 0.0 or close to 0.0)\")"]},{"cell_type":"markdown","metadata":{"id":"wCnb9WhBte-r"},"source":["## (3) Implementing the Embedding Layer Loss"]},{"cell_type":"markdown","metadata":{"id":"rUMedWUOte_N"},"source":["This part is located in EmbedLayerLoss in kd_loss.py. You must implement the call function of the class. You will need to implement the formula (9) in section 3 of __[TinyBERT](https://arxiv.org/pdf/1909.10351.pdf)__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFegY_gNte_R"},"outputs":[],"source":["from kd_loss import EmbeddingLayerLoss\n","teacher_embed_dim = 50\n","student_embed_dim = 10\n","batch_size = 2\n","\n","with open(root_folder+\"kd_checks/kd_embed_loss.json\",'r') as f:\n","  io = json.load(f)\n","  teacher_embd = th.tensor(io['teacher_embed'])\n","  student_embd = th.tensor(io['student_embed'])\n","  expected_output = th.tensor(io['expected_output'])\n","\n","embd_loss = EmbeddingLayerLoss(teacher_embed_dim,student_embed_dim)\n","embd_loss.load_state_dict(th.load(root_folder+\"kd_checks/kd_embed_loss\"))\n","output = embd_loss(teacher_embd, student_embd)\n","validate_to_array(model_out_to_list,((teacher_embd,student_embd),embd_loss),'kdembdloss', root_folder)\n","print(\"Total error on the output:\",th.sum(th.abs(expected_output-output)).item(), \"(should be 0.0 or close to 0.0)\")"]},{"cell_type":"markdown","metadata":{"id":"-LFwH1IPvsNV"},"source":["## (4) Implementing the Prediction Loss"]},{"cell_type":"markdown","metadata":{"id":"woCYlCLlvsN3"},"source":["This part is located in PredictionLoss in kd_loss.py. You must implement the call function of the class. You will need to implement the formula (10) in section 3 of __[TinyBERT](https://arxiv.org/pdf/1909.10351.pdf)__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bsHNe0iWvsN6"},"outputs":[],"source":["from kd_loss import PredictionLoss\n","word_count = 10\n","batch_size = 2\n","\n","with open(root_folder+\"kd_checks/kd_pred_loss.json\",'r') as f:\n","  io = json.load(f)\n","  teacher_pred = th.tensor(io['teacher_pred'])\n","  student_pred = th.tensor(io['student_pred'])\n","  expected_output = th.tensor(io['expected_output'])\n","\n","pred_loss = PredictionLoss()\n","output = pred_loss(teacher_pred, student_pred)\n","validate_to_array(model_out_to_list,((teacher_pred,student_pred),pred_loss),'kdpredloss', root_folder)\n","print(\"Total error on the output:\",th.sum(th.abs(expected_output-output)).item(), \"(should be 0.0 or close to 0.0)\")"]},{"cell_type":"markdown","metadata":{"id":"Z0Fm6jwlyRnU"},"source":["## (5) Implementing the Knowledge Distillation Loss"]},{"cell_type":"markdown","metadata":{"id":"2m5gjJyOyRn3"},"source":["This part is located in KnowledgeDistillationLoss in kd_loss.py. You must implement the call function of the class. You will need to implement the formula (11) in section 3 of __[TinyBERT](https://arxiv.org/pdf/1909.10351.pdf)__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yq2GI2UUyRn7"},"outputs":[],"source":["from kd_loss import KnowledgeDistillationLoss\n","num_channels = 12\n","teacher_hidden_dim = 60\n","student_hidden_dim = 15\n","teacher_embed_dim = 50\n","student_embed_dim = 10\n","word_count = 5\n","teacher_num_blocks = 6\n","student_num_blocks = 2\n","batch_size = 2\n","layer_mapping = range(2,6,3)\n","\n","with open(root_folder+\"kd_checks/kd_loss.json\",'r') as f:\n","  io = json.load(f)\n","  teacher_out = io['teacher_out']\n","  student_out = io['student_out']\n","  teacher_out = dict(\n","      embeddings=th.tensor(teacher_out['embeddings']),\n","      attentions=[th.tensor(o) for o in teacher_out['attentions']],\n","      hidden_states=[th.tensor(o) for o in teacher_out['hidden_states']],\n","      logits=th.tensor(teacher_out['embeddings'])\n","  )\n","  student_out = dict(\n","      embeddings=th.tensor(student_out['embeddings']),\n","      attentions=[th.tensor(o) for o in student_out['attentions']],\n","      hidden_states=[th.tensor(o) for o in student_out['hidden_states']],\n","      logits=th.tensor(student_out['embeddings'])\n","  )\n","  expected_output = th.tensor(io['expected_output'])\n","\n","kd_loss = KnowledgeDistillationLoss(teacher_embed_dim,student_embed_dim,teacher_hidden_dim,student_hidden_dim,layer_mapping)\n","kd_loss.load_state_dict(th.load(root_folder+\"kd_checks/kd_loss\"))\n","output = kd_loss(teacher_out, student_out)\n","validate_to_array(model_out_to_list,((teacher_out,student_out),kd_loss),'kdloss', root_folder)\n","print(\"Total error on the output:\",th.sum(th.abs(expected_output-output)).item(), \"(should be 0.0 or close to 0.0)\")"]},{"cell_type":"markdown","metadata":{"id":"9ZFUAhfr-p8q"},"source":["#Experimental Setup"]},{"cell_type":"markdown","metadata":{"id":"JbRwa2Dips0O"},"source":["##General Distillation"]},{"cell_type":"markdown","metadata":{"id":"YYI7sTo8DiZa"},"source":["###Data Loading"]},{"cell_type":"markdown","metadata":{"id":"E_E6d-DN-p8y"},"source":["Below is the text parsing set up. We will be using the wikitext dataset as used in the paper. But since we are just demonstrating the method, we will use the small wikitext-2 dataset instead of the standard wikitext-103 set. Wikitext contains thousands of cleaned English Wikipedia articles separated by sentence. Since the order of sentences is left in tact, the dataset can be used to model long term dependencies between words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoH6OWg0-p8z"},"outputs":[],"source":["%%capture\n","from datasets import load_dataset\n","datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"]},{"cell_type":"markdown","metadata":{"id":"JGhXJ86gDr_W"},"source":["See that the data has been split into training, validation, and testing sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96VeMWrs-p80"},"outputs":[],"source":["print(datasets)"]},{"cell_type":"markdown","metadata":{"id":"nOPPgsaGGLHb"},"source":["A sample of the dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQODO5ZMDzxo"},"outputs":[],"source":["print(\"\".join(datasets['train'][:100]['text']))"]},{"cell_type":"markdown","metadata":{"id":"CCqAtGMyGQoo"},"source":["The words must be parsed and hashed according to the vocabulary of our model. Instead of masking sentences to equal length, this time we will separate the contiguous text sequence into equal size blocks, possibly breaking up whole sentences."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yK5jjAPu-p82"},"outputs":[],"source":["%%capture\n","tokenized_datasets = datasets.map(lambda samples: tokenizer(samples['text']), batched=True, num_proc=4, remove_columns=[\"text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1lM3fWe-p84"},"outputs":[],"source":["block_size = 128\n","def group_texts(examples):\n","    # Concatenate all texts.\n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n","        # customize this part to your needs.\n","    total_length = (total_length // block_size) * block_size\n","    # Split by chunks of max_len.\n","    result = {\n","        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQhOTXOi-p87"},"outputs":[],"source":["%%capture\n","lm_datasets = tokenized_datasets.map(\n","    group_texts,\n","    batched=True,\n","    batch_size=1000,\n","    num_proc=4,\n","    load_from_cache_file=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3-FYlcd-p89"},"outputs":[],"source":["tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzsJJQ4S-p8_"},"outputs":[],"source":["lm_datasets[\"train\"][1].keys()"]},{"cell_type":"markdown","metadata":{"id":"errlsVm6A_tC"},"source":["### Set Up Student Model"]},{"cell_type":"markdown","metadata":{"id":"IRhmgXFo-p8u"},"source":["Fill in the dimensions of BERT and the student network as specified in section 4.2 of the paper"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLpqNBPD-p8v"},"outputs":[],"source":["vocab_size = int(1e4)\n","teacher_hddn_dim = \n","student_hddn_dim = \n","teacher_num_hddn_layers = \n","student_num_hddn_layers =  \n","teacher_num_attn_heads = \n","student_num_attn_heads = \n","teacher_ff_dim = \n","student_ff_dim = \n","teacher_embd_dim = \n","student_embd_dim = \n","layer_mapping = range(,\n","                      ,\n","                      )\n","\n","student_config = BertConfig(\n","    hidden_size=student_hddn_dim,\n","    num_hidden_layers=student_num_hddn_layers,\n","    num_attention_heads=student_num_attn_heads,\n","    intermediate_size=student_ff_dim,\n",")"]},{"cell_type":"markdown","metadata":{"id":"NhOW156Br6Y1"},"source":["###Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoLfKJ5wSuoD"},"outputs":[],"source":["from kd_loss import KnowledgeDistillationLoss\n","teacher_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n","teacher_model.load_state_dict(th.load(root_folder+'bert_models/teacher_wikitext.pt'))\n","student_model = BertForMaskedLM(student_config).to(device)\n","criterion = KnowledgeDistillationLoss(teacher_embd_dim,student_embd_dim,teacher_hddn_dim,student_hddn_dim,layer_mapping).to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTwV3Lwc-p9E"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","import gc\n","gc.collect()\n","optimizer = optim.Adam(params=student_model.parameters(),lr=5e-5,weight_decay=0.01)\n","student_model.to(device)\n","lr = 1e-4\n","batch_size = 10\n","epochs=10\n","for epoch in range(epochs):\n","    lm_datasets[\"train\"].shuffle(load_from_cache_file=False)\n","    t = tqdm(range(0,len(lm_datasets[\"train\"]),batch_size))\n","    accuracies = []\n","    losses = []\n","    for i in t:\n","        data = lm_datasets[\"train\"][i:i+batch_size]\n","        data = {k: th.tensor(v).to(device) for k,v in data.items()}\n","        teacher_out = teacher_model(**data,**forward_kwargs)\n","        student_out = student_model(**data,**forward_kwargs)\n","        teacher_out['embeddings'] = teacher_model.get_input_embeddings().weight\n","        student_out['embeddings'] = student_model.get_input_embeddings().weight\n","        loss = criterion(teacher_out,student_out,penalize_prediction=False)\n","        losses.append(loss.detach().cpu().numpy())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        accuracy = th.eq(student_out['logits'].argmax(dim=2,keepdim=False).float(),data['labels']).float().mean()\n","        accuracies.append(accuracy.detach().cpu().numpy())\n","        loss = np.around(np.mean(losses[-100:]),3)\n","        accuracy = np.around(np.mean(accuracies[-100:]),2)\n","        t.set_description(\"Epoch: \"+str(epoch)+\" Loss: \"+str(loss))\n","    os.makedirs(root_folder+'bert_models',exist_ok=True)\n","    th.save(student_model.state_dict(),root_folder+'bert_models/student_wikitext.pt')"]},{"cell_type":"markdown","metadata":{"id":"6PMK0aI-os6U"},"source":["We next train a control, which is just the same BERT shaped student model trained from scratch on general and task specific data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NiJZ7nXajRl3"},"outputs":[],"source":["control_model = BertForMaskedLM(student_config).to(device)\n","gc.collect()\n","control_model.train()\n","optimizer = optim.Adam(params=student_model.parameters(),lr=lr,weight_decay=0.01)\n","lr = 1e-4\n","batch_size = 10\n","epochs=1\n","for epoch in range(epochs):\n","    lm_datasets[\"train\"].shuffle()\n","    t = tqdm(range(0,len(lm_datasets[\"train\"]),batch_size))\n","    accuracies = []\n","    losses = []\n","    for i in t:\n","        data = lm_datasets[\"train\"][i:i+batch_size]\n","        data = {k: th.tensor(v).to(device) for k,v in data.items()}\n","        student_out = student_model(**data,**forward_kwargs)\n","        losses.append(student_out['loss'].detach().cpu().numpy())\n","        \n","        optimizer.zero_grad()\n","        student_out['loss'].backward()\n","        optimizer.step()\n","        accuracy = th.eq(student_out['logits'].argmax(dim=2,keepdim=False).float(),data['labels']).float().mean()\n","        accuracies.append(accuracy.detach().cpu().numpy())\n","        loss = np.around(np.mean(losses[-100:]),3)\n","        accuracy = np.around(np.mean(accuracies[-100:]),2)\n","        t.set_description(\"Epoch: \"+str(epoch)+\" Loss: \"+str(loss))\n","    os.makedirs(root_folder+'bert_models',exist_ok=True)\n","    th.save(control_model.state_dict(),root_folder+'bert_models/control_wikitext.pt')"]},{"cell_type":"markdown","metadata":{"id":"GSGZJmYIpZdq"},"source":["##Task Specific Distillation"]},{"cell_type":"markdown","metadata":{"id":"USC3GiVeqZMN"},"source":["###Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjJWERd7-p9Q","scrolled":false},"outputs":[],"source":["%%capture\n","from datasets import load_dataset\n","datasets = load_dataset('glue', 'mrpc')"]},{"cell_type":"markdown","metadata":{"id":"YIf7dyt8qnw-"},"source":["See that the data has been split into training, validation, and testing sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dONZeNeI-p9S"},"outputs":[],"source":["datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87VliGivqv7Q"},"outputs":[],"source":["print(datasets['train'][0]['sentence1'], datasets['train'][0]['sentence2'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqLU14b_-p9U"},"outputs":[],"source":["%%capture\n","mrpc_tok = datasets.map(lambda samples: tokenizer(samples['sentence1'], samples['sentence2'],padding='max_length',max_length=150),\n","                       remove_columns=['sentence1', 'sentence2','idx'],\n","                       load_from_cache_file=False,\n","                      )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHHVEVkT-p9V"},"outputs":[],"source":["%%capture\n","def filter_texts(examples):\n","    examples[\"labels\"] = examples[\"label\"].copy()\n","    examples.pop('label',None)\n","    return examples\n","mrpc = mrpc_tok.map(\n","    filter_texts,\n","    batched=True,\n","    batch_size=1000,\n","    num_proc=4,\n","    load_from_cache_file=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"n58bP7PKrnJ1"},"source":["###Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhg-b7tmsA3V"},"outputs":[],"source":["from kd_loss import KnowledgeDistillationLoss\n","from transformers import BertForNextSentencePrediction, BertForSequenceClassification\n","teacher_model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\").to(device)\n","teacher_model.load_state_dict(th.load(root_folder+'bert_models/teacher_mrpc.pt'))\n","student_model = BertForNextSentencePrediction(student_config).to(device)\n","student_model.load_state_dict(th.load(root_folder+'bert_models/student_wikitext.pt'),strict=False)\n","criterion = KnowledgeDistillationLoss(teacher_embd_dim,student_embd_dim,teacher_hddn_dim,student_hddn_dim,layer_mapping).to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4qnKJsRsA3w"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","import gc\n","gc.collect()\n","optimizer = optim.Adam(params=student_model.parameters(),lr=5e-5,weight_decay=0.01)\n","student_model.to(device)\n","lr = 1e-4\n","batch_size = 10\n","epochs=10\n","for epoch in range(epochs):\n","    mrpc[\"train\"].shuffle(load_from_cache_file=False)\n","    t = tqdm(range(0,len(mrpc[\"train\"]),batch_size))\n","    accuracies = []\n","    losses = []\n","    for i in t:\n","        data = mrpc[\"train\"][i:i+batch_size]\n","        data = {k: th.tensor(v).to(device) for k,v in data.items()}\n","        teacher_out = teacher_model(**data,**forward_kwargs)\n","        student_out = student_model(**data,**forward_kwargs)\n","        teacher_out['embeddings'] = teacher_model.get_input_embeddings().weight\n","        student_out['embeddings'] = student_model.get_input_embeddings().weight\n","        loss = criterion(teacher_out,student_out,penalize_prediction=True)\n","        losses.append(loss.detach().cpu().numpy())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        accuracy = th.eq(student_out['logits'].argmax(dim=1,keepdim=False).float(),data['labels']).float().mean()\n","        accuracies.append(accuracy.detach().cpu().numpy())\n","        loss = np.around(np.mean(losses[-100:]),3)\n","        accuracy = np.around(np.mean(accuracies[-100:]),2)\n","        t.set_description(\"Epoch: \"+str(epoch)+\" Loss: \"+str(loss)+\" Accuracy: \"+str(accuracy))\n","    os.makedirs(root_folder+'bert_models',exist_ok=True)\n","    th.save(student_model.state_dict(),root_folder+'bert_models/student_mrpc.pt')"]},{"cell_type":"markdown","metadata":{"id":"vdYyORtusA30"},"source":["We next train a control, which is just the same BERT shaped student model trained from scratch on general and task specific data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PeOX276S-p9f"},"outputs":[],"source":["control_model = BertForNextSentencePrediction(student_config).to(device)\n","control.load_state_dict(th.load(root_folder+'bert_models/control_wikitext.pt'),strict=False)\n","gc.collect()\n","control_model.train()\n","optimizer = optim.Adam(params=student_model.parameters(),lr=lr,weight_decay=0.01)\n","lr = 1e-4\n","batch_size = 10\n","epochs=1\n","for epoch in range(epochs):\n","    mrpc[\"train\"].shuffle()\n","    t = tqdm(range(0,len(mrpc[\"train\"]),batch_size))\n","    accuracies = []\n","    losses = []\n","    for i in t:\n","        data = mrpc[\"train\"][i:i+batch_size]\n","        data = {k: th.tensor(v).to(device) for k,v in data.items()}\n","        student_out = student_model(**data,**forward_kwargs)\n","        losses.append(student_out['loss'].detach().cpu().numpy())\n","        \n","        optimizer.zero_grad()\n","        student_out['loss'].backward()\n","        optimizer.step()\n","        accuracy = th.eq(student_out['logits'].argmax(dim=2,keepdim=False).float(),data['labels']).float().mean()\n","        accuracies.append(accuracy.detach().cpu().numpy())\n","        loss = np.around(np.mean(losses[-100:]),3)\n","        accuracy = np.around(np.mean(accuracies[-100:]),2)\n","        t.set_description(\"Epoch: \"+str(epoch)+\" Loss: \"+str(loss))\n","    os.makedirs(root_folder+'bert_models',exist_ok=True)\n","    th.save(control_model.state_dict(),root_folder+'bert_models/control_wikitext.pt')"]},{"cell_type":"markdown","metadata":{"id":"P0osHIcMCbFu"},"source":["# Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"JJlXrT73Cjgn"},"source":["##BERT Wikitext Specific Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSfBjx6eCPCr"},"outputs":[],"source":["%%capture\n","from datasets import load_dataset\n","datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n","tokenized_datasets = datasets.map(lambda samples: tokenizer(samples['text']), batched=True, num_proc=4, remove_columns=[\"text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zaKg2I6xCPCs"},"outputs":[],"source":["block_size = 128\n","def group_texts(examples):\n","    # Concatenate all texts.\n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n","        # customize this part to your needs.\n","    total_length = (total_length // block_size) * block_size\n","    # Split by chunks of max_len.\n","    result = {\n","        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qaFbanmCPCt"},"outputs":[],"source":["%%capture\n","lm_datasets = tokenized_datasets.map(\n","    group_texts,\n","    batched=True,\n","    batch_size=1000,\n","    num_proc=4,\n","    load_from_cache_file=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LwOUQXDLCPCz"},"outputs":[],"source":["from tqdm.autonotebook import tqdm\n","import gc\n","gc.collect()\n","teacher_model.train()\n","optimizer = optim.Adam(params=teacher_model.parameters(),lr=lr,weight_decay=0.01)\n","lr = 1e-4\n","batch_size = 10\n","epochs=1\n","for epoch in range(epochs):\n","    lm_datasets[\"train\"].shuffle()\n","    t = tqdm(range(0,len(lm_datasets[\"train\"]),batch_size))\n","    accuracies = []\n","    losses = []\n","    for i in t:\n","        data = lm_datasets[\"train\"][i:i+batch_size]\n","        data = {k: th.tensor(v).to(device) for k,v in data.items()}\n","        teacher_out = teacher_model(**data,**forward_kwargs)\n","        losses.append(teacher_out['loss'].detach().cpu().numpy())\n","        \n","        optimizer.zero_grad()\n","        teacher_out['loss'].backward()\n","        optimizer.step()\n","        accuracy = th.eq(teacher_out['logits'].argmax(dim=2,keepdim=False).float(),data['labels']).float().mean()\n","        accuracies.append(accuracy.detach().cpu().numpy())\n","        loss = np.around(np.mean(losses[-100:]),3)\n","        accuracy = np.around(np.mean(accuracies[-100:]),2)\n","        t.set_description(\"Epoch: \"+str(epoch)+\" Loss: \"+str(loss)+\" Accuracy: \"+str(accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljmtYeeoCPC0"},"outputs":[],"source":["teacher_model.eval()\n","lm_datasets[\"validation\"].shuffle()\n","t = tqdm(range(0,len(lm_datasets[\"validation\"]),batch_size))\n","for i in t:\n","    data = lm_datasets[\"validation\"][i:i+batch_size]\n","    data = {k: th.tensor(v).to(device) for k,v in data.items()}\n","    teacher_out = teacher_model(**data,**forward_kwargs)\n","    losses.append(teacher_out['loss'].detach().cpu().numpy())\n","    \n","    accuracy = th.eq(teacher_out['logits'].argmax(dim=2,keepdim=False).float(),data['labels']).float().mean()\n","    accuracies.append(accuracy.detach().cpu().numpy())\n","    loss = np.around(np.mean(losses),3)\n","    accuracy = np.around(np.mean(accuracies),2)\n","    t.set_description(\"Validation - \"+\"Loss: \"+str(loss)+\" Accuracy: \"+str(accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5EBMMfBCPC1"},"outputs":[],"source":["os.makedirs(root_folder+'bert_models',exist_ok=True)\n","th.save(teacher_model.state_dict(),root_folder+'bert_models/teacher_wikitext.pt')"]},{"cell_type":"markdown","metadata":{"id":"w4wghDKGvWRm"},"source":["##BERT MRPC Specific Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJstjDEivWw8"},"outputs":[],"source":["%%capture\n","from datasets import load_dataset\n","datasets = load_dataset('glue', 'mrpc')\n","mrpc_tok = datasets.map(lambda samples: tokenizer(samples['sentence1'], samples['sentence2'],padding='max_length',max_length=150),\n","                       remove_columns=['sentence1', 'sentence2','idx'],\n","                       load_from_cache_file=False,\n","                      )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XH2GqNSXvWw9"},"outputs":[],"source":["%%capture\n","def filter_texts(examples):\n","    examples[\"labels\"] = examples[\"label\"].copy()\n","    examples.pop('label',None)\n","    return examples\n","mrpc = mrpc_tok.map(\n","    filter_texts,\n","    batched=True,\n","    batch_size=1000,\n","    num_proc=4,\n","    load_from_cache_file=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PnGTEIORvWw_"},"outputs":[],"source":["%%capture\n","from transformers import BertForNextSentencePrediction, BertForSequenceClassification\n","teacher_model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\").to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YiwILDOpv1pH"},"outputs":[],"source":["from tqdm.autonotebook import tqdm\n","import gc\n","gc.collect()\n","teacher_model.train()\n","lr = 2e-5\n","batch_size = 10\n","epochs=10\n","optimizer = optim.Adam(params=teacher_model.parameters(),lr=lr,weight_decay=0.01)\n","for epoch in range(epochs):\n","    mrpc[\"train\"].shuffle(load_from_cache_file=False)\n","    t = tqdm(range(0,len(mrpc[\"train\"]),batch_size))\n","    accuracies = []\n","    losses = []\n","    for i in t:\n","        data = mrpc[\"train\"][i:i+batch_size]\n","        data = {k: th.tensor(v).to(device) for k,v in data.items()}\n","        teacher_out = teacher_model(**data,**forward_kwargs)\n","        losses.append(teacher_out['loss'].detach().cpu().numpy())\n","        \n","        optimizer.zero_grad()\n","        teacher_out['loss'].backward()\n","        optimizer.step()\n","        accuracy = th.eq(teacher_out['logits'].argmax(dim=1,keepdim=False).float(),data['labels']).float().mean()\n","        accuracies.append(accuracy.detach().cpu().numpy())\n","        loss = np.around(np.mean(losses[-100:]),3)\n","        accuracy = np.around(np.mean(accuracies[-100:]),2)\n","        t.set_description(\"Epoch: \"+str(epoch)+\" Loss: \"+str(loss)+\" Accuracy: \"+str(accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGBVGfo7v1pJ"},"outputs":[],"source":["teacher_model.eval()\n","mrpc[\"validation\"]\n","t = tqdm(range(0,len(mrpc[\"validation\"]),batch_size))\n","for i in t:\n","    data = mrpc[\"validation\"][i:i+batch_size]\n","    data = {k: th.tensor(v).to(device) for k,v in data.items()}\n","    teacher_out = teacher_model(**data,**forward_kwargs)\n","    losses.append(teacher_out['loss'].detach().cpu().numpy())\n","    \n","    accuracy = th.eq(teacher_out['logits'].argmax(dim=1,keepdim=False).float(),data['labels']).float().mean()\n","    accuracies.append(accuracy.detach().cpu().numpy())\n","    loss = np.around(np.mean(losses),3)\n","    accuracy = np.around(np.mean(accuracies),2)\n","    t.set_description(\"Validation - \"+\"Loss: \"+str(loss)+\" Accuracy: \"+str(accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuenOJXTv1pL"},"outputs":[],"source":["os.makedirs(root_folder+'bert_models',exist_ok=True)\n","th.save(teacher_model.state_dict(),root_folder+'bert_models/teacher_mrpc.pt')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["u3vhv5zl-p7z","P0osHIcMCbFu"],"name":"3 Knowledge Distillation.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}